# Phase 3: GRPO Training Configuration

# Model configuration
model:
  base: "Qwen/Qwen2.5-Coder-32B-Instruct"
  cache_dir: "/home/user/.cache/huggingface"
  revision: "main"

# QLoRA configuration
qlora:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

  lora_r: 64
  lora_alpha: 16
  lora_dropout: 0.05
  lora_bias: "none"

  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Training arguments (Stage 1)
training_stage1:
  output_dir: "/home/user/work/rubric-rewards-training/stage1"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch size = 16
  learning_rate: 5.0e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01

  # Performance optimizations
  bf16: true
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
  max_grad_norm: 1.0

  # Logging & saving
  logging_steps: 10
  save_steps: 100
  save_total_limit: 3
  eval_steps: 50
  evaluation_strategy: "steps"

  # Hardware
  dataloader_num_workers: 4
  dataloader_pin_memory: true

# Training arguments (Stage 2)
training_stage2:
  output_dir: "/home/user/work/rubric-rewards-training/stage2"
  num_train_epochs: 2
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-5  # Lower LR for fine-tuning
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01

  # Same as stage 1
  bf16: true
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
  max_grad_norm: 1.0

  logging_steps: 10
  save_steps: 50
  save_total_limit: 3
  eval_steps: 25
  evaluation_strategy: "steps"

  dataloader_num_workers: 4
  dataloader_pin_memory: true

# GRPO algorithm parameters
grpo:
  num_generations: 4  # N plans per goal
  frozen_grader: true

  # Length control (from paper)
  max_thinking_tokens: 2048
  max_plan_words: 500
  length_penalty: 0.5  # Penalty per excess word

  # Reward computation
  rubric_weight: 0.7
  length_weight: 0.2
  diversity_weight: 0.1

  # Temperature for generation
  temperature: 0.7
  top_p: 0.9

  # Grading prompts
  use_general_guidelines: true  # Add 7 general guidelines to rubric grading

# Data configuration
data:
  database_path: "/home/user/mcp_servers/polymax-synthesizer/papers.db"
  validation_split: "data/validation_split.json"
  max_train_samples: null  # null = use all
  max_eval_samples: 60

# Monitoring
monitoring:
  tensorboard: true
  tensorboard_dir: "outputs/logs/tensorboard"
  log_grading_examples: true
  log_examples_per_epoch: 5
  track_vram: true
  vram_check_interval: 100  # steps
